---
title: "Prediction Assignment Writeup"
author: "Carlos Chunga"
date: "31/1/2021"
output: html_document
---

## Introduction

This is the course project for the "Practical Machine Learning" course on Coursera by Johns Hopkins University.

As explained in the instructions: 

> The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.

More information about the data set can be found [here](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har).

## Data Preparation

First, we load the data sets as well as some necessary libraries:

```{r message = FALSE, warning = FALSE, results = 'hide'}
library(tidyverse)
library(caret)
library(randomForest)

training_data <- read.csv("pml-training.csv")
validation_data <- read.csv("pml-testing.csv")
```

We see that the training data set has dimension `r dim(training_data)`, while the validation set has `r dim(validation_data)`. This means we have 159 variables available to predict the "classe" response variable.

The focus of this exercise will be on ensemble methods. As outlined during the course, ensemble machine learning algorithms such as random forests (based on a large number of individual decision trees), and boosting (e.g. AdaBoost, based on weak learners such as "stumps" which are basically decision trees with just one node and two leaves) tend to have a good performance, and small out-of-sample errors. Specifically, I will be using a random forests.

Before moving on, we need to see which variables have missing values and decide whether to impute those missing values or to eliminate the corresponding columns altogether. Here, I decide to drop those columns:

```{r}
training_data <- training_data %>% select_if(~ !any(is.na(.)))
validation_data <- validation_data %>% select_if(~ !any(is.na(.)))
```

Now, I partition the "training" data into 70% training and 30% testing. The "validation" set will be used later on for the final prediction exercise. Before partitioning the data, I convert the "classe" variable from character to factor:

```{r}
training_data$classe <- as.factor(training_data$classe)

set.seed(1503)

inTrain <- createDataPartition(training_data$classe, p = 0.7, list = FALSE)
training <- training_data[inTrain,]
testing <- training_data[-inTrain,]
```

Before training the model, I check if there are more variables we could discard with the help of the `nearZeroVar()` function from "caret", which flags those variables with almost no variance within them. After flagging those variables, I proceed to drop them:

```{r}
nzv_variables <- nearZeroVar(training)
training <- training[, -nzv_variables]
testing  <- testing[, -nzv_variables]
dim(training); dim(testing)
```

## Model Building: Random Forest with Method = 'Ranger'

Now, we would like to fit a random forest to predict the "classe" variable. We could combine different algorithms, but I will only be using random forests here, since the former approach is extremely time-consuming during the training phase.

According to the authors of the `randomForest` package, [Leo Breiman & Adele Cutler](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr), cross-validation may not be necessary for random forests:

> In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run, as follows:

> Each tree is constructed using a different bootstrap sample from the original data. About one-third of the cases are left out of the bootstrap sample and not used in the construction of the kth tree.

> Put each case left out in the construction of the kth tree down the kth tree to get a classification. In this way, a test set classification is obtained for each case in about one-third of the trees. At the end of the run, take j to be the class that got most of the votes every time case n was oob. The proportion of times that j is not equal to the true class of n averaged over all cases is the oob error estimate. This has proven to be unbiased in many tests.

If we use "rf" as the method for the `train()` function, this will take too long (+30 mins). Thus, it is more convenient to use "ranger" as the method. According to the R Documentation:

> Ranger is a fast implementation of random forests (Breiman 2001) or recursive partitioning, particularly suited for high dimensional data. Classification, regression, and survival forests are supported. Classification and regression forests are implemented as in the original Random Forest (Breiman 2001), survival forests as in Random Survival Forests (Ishwaran et al. 2008). Includes implementations of extremely randomized trees (Geurts et al. 2006) and quantile regression forests (Meinshausen 2006).

Now we proceed to build and train the random forest model:

```{r}
set.seed(2203)

modFit <- train(classe ~ ., method = "ranger", data = training)
modFit$finalModel
```

## Measuring the Accuracy

Then, we compare our model predictions with the testing data set to measure the accuracy. **I expect that the out-of-sample error will be close to 0 because we're using an ensemble learning method for classification (random forest), known for its outstanding performance compared to simpler algorithms:**

```{r}
predict_rf <- predict(modFit, newdata = testing)
cm_rf <- confusionMatrix(predict_rf, testing$classe)
cm_rf
```

We observe a perfect accuracy of 1,00. This may be due to some sort of overfitting.

Finally, we use the fitted model on our original validation_data of 20 observations. These results will be used to answer the Course Project Prediction Quiz. Due to confidentiality, the answers will not be explicitly shown here.

```{r}
results <- predict(modFit, newdata = validation_data)
```